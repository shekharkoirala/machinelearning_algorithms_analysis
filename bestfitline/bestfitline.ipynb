{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bestfitline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharkoirala/machinelearning_algorithms_analysis/blob/master/bestfitline/bestfitline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d6Gnvkatr-WH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression analysis**"
      ]
    },
    {
      "metadata": {
        "id": "DSmhHRd1sL7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Using sklearn**\n",
        "\n",
        "**step 1: Download the diabetes data from source**"
      ]
    },
    {
      "metadata": {
        "id": "_KtlU4IAZZOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download the data\n",
        "!wget https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/diabetes.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNlXM5c1avBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "%matplotlib inline\n",
        "\n",
        "#datatype 1\n",
        "df = pd.read_csv(\"diabetes.csv\")\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UebNhQDbOvv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Outcome'], df['Outcome'], stratify=df['Outcome'], random_state=66)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtuH7UuAbVxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression().fit(X_train, y_train)\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-u1xOc1cfHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict = logreg.predict(X_test)\n",
        "predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nALK5B0dszNV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://scikit-learn.org/stable/_images/sphx_glr_plot_logistic_001.png)"
      ]
    },
    {
      "metadata": {
        "id": "6RyQ8v8XsmDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Plotting the graph ,  but our data is not in the following format**\n",
        "\n",
        "**Our data has multiple features and comparing any one of the features with outcomes , we have not dictinctive features as above,**\n"
      ]
    },
    {
      "metadata": {
        "id": "T8C90pDrtB0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X_train['Insulin'], y_train, edgecolor='k')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9r6E087tVqe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**so lets try PCA to reduce the dimension of dataset , and see , if the features becomes distinctive**"
      ]
    },
    {
      "metadata": {
        "id": "bSObG_6ychXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(X_test)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['data1', 'data2'])\n",
        "\n",
        "plt.scatter(principalDf['data2'], y_test, edgecolor='k')\n",
        "# plt.plot(principalDf['data2'], predict, color='blue', linewidth=3)\n",
        "\n",
        "# plt.xticks(())\n",
        "# plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbyXGWBAt4P_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**So , the traditional best fit line approach is not suitable in logistic regression , rather than plotting the line, we can  scatter plot it. \n",
        "Red plots are valid test data , and blue plots are predicted test data**"
      ]
    },
    {
      "metadata": {
        "id": "yc6Vg8Z_oDyR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "fig = plt.figure(figsize=(25, 18), dpi=80)\n",
        "n = 100\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "xs = principalDf['data1']\n",
        "ys = principalDf['data2']\n",
        "zs = y_test\n",
        "ax.scatter(xs, ys, zs, c=\"r\", alpha = 0.7, marker= \"o\")\n",
        "xs = principalDf['data1']\n",
        "ys = principalDf['data2']\n",
        "zs = predict\n",
        "ax.scatter(xs, ys, zs, c=\"b\", alpha = 0.7, marker = '^')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "496IuerHuyWj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Logistic regression in Tensorflow**"
      ]
    },
    {
      "metadata": {
        "id": "SfT7b7wSFQYv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Vanilla model**"
      ]
    },
    {
      "metadata": {
        "id": "9oWngDeXu0PK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "#parameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "n_epochs = 30\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "\n",
        "# step 1 : Read data\n",
        "mnist_folder = 'data/mnist'\n",
        "mnist = input_data.read_data_sets(mnist_folder, one_hot=True)\n",
        "X_batch , Y_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "\n",
        "#step 2 : ceate placeholder s\n",
        "X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
        "Y = tf.placeholder(tf.int32, [batch_size,10], name=\"label\")\n",
        "\n",
        "\n",
        "# step 3 : initialize weight and bias\n",
        "# weight : initialize weight mean : 0 , std-dev : 0.1\n",
        "# tf.random_normal_initializer()\n",
        "# bias : 0.0 , tf.zeros_initializer()\n",
        "# img = (?,784) * weight = (784,10) = (?,10) + bias (1,10)\n",
        "w = tf.get_variable(\"weight\", shape=(784, 10),\n",
        "                    initializer=tf.random_normal_initializer(mean=0,\n",
        "                                                             stddev=0.1))\n",
        "b = tf.get_variable(\"bias\", shape=(1, 10), initializer=\n",
        "                    tf.zeros_initializer())\n",
        "\n",
        "\n",
        "# step 4: model , logits , model that return logits\n",
        "logits = tf.matmul(X, w) + b\n",
        "\n",
        "#step 5 : softmax\n",
        "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y ,\n",
        "                                                  name = \"loss\")\n",
        "loss = tf.reduce_mean(entropy)\n",
        "\n",
        "\n",
        "# st\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "preds = tf.nn.softmax(logits)\n",
        "correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "writer = tf.summary.FileWriter('./graphs/logreg_placeholder',\n",
        "                               tf.get_default_graph())\n",
        "\n",
        "with tf.name_scope(\"summaries_loss\"):\n",
        "    tf.summary.scalar(\"loss\", loss)\n",
        "    tf.summary.histogram(\"loss histogram\", loss)\n",
        "    summary_op1 = tf.summary.merge_all()\n",
        "\n",
        "with tf.name_scope(\"summaries_accuracy\"):\n",
        "    tf.summary.scalar(\"accuracy\", accuracy)\n",
        "    tf.summary.histogram(\"accuracy histogram\", accuracy)\n",
        "    summary_op2 = tf.summary.merge_all()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    n_batches = int(mnist.train.num_examples/batch_size)\n",
        "    for i in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        for j in range(n_batches):\n",
        "            X_batch , Y_batch = mnist.train.next_batch(batch_size)\n",
        "            _, loss_batch, summary = sess.run([optimizer, loss, summary_op1],\n",
        "                                      feed_dict={X:X_batch, Y:Y_batch})\n",
        "            total_loss += loss_batch\n",
        "        writer.add_summary(summary, global_step=i)\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "    print('Total time: {0} seconds'.format(time.time() -start))\n",
        "\n",
        "    total_correct_preds = 0\n",
        "    n_batches = int(mnist.test.num_examples / batch_size)\n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "        accuracy_batch, summary = sess.run([accuracy, summary_op2], feed_dict={\n",
        "            X:X_batch,Y:Y_batch})\n",
        "        total_correct_preds +=accuracy_batch\n",
        "        writer.add_summary(summary, global_step=i)\n",
        "\n",
        "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
        "\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3WNRWn6cFVa2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model's ***accuracy*** could be improved by using , one more **hidden layer** and** regularization **\n",
        "\n",
        "The model's ***time*** could be improved by using **dataset** instead of **placeholder.**"
      ]
    },
    {
      "metadata": {
        "id": "2KpdLekTGTn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import urllib\n",
        "import gzip\n",
        "import shutil\n",
        "import struct\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "def read_birth_life_data(file_name):\n",
        "    with open(file_name) as f:\n",
        "        text = f.read().splitlines()[1:]\n",
        "        data = [line.split('\\t') for line in text]\n",
        "        births = [float(line[1]) for line in data]\n",
        "        lifes = [float(line[2]) for line in data]\n",
        "        data = list(zip(births, lifes))\n",
        "        n_samples = len(data)\n",
        "        data = np.asarray(data, dtype=np.float32)\n",
        "    return data, n_samples\n",
        "\n",
        "\n",
        "def download_one_file(download_url,\n",
        "                    local_dest,\n",
        "                    expected_byte=None,\n",
        "                    unzip_and_remove=False):\n",
        "    \"\"\"\n",
        "    Download the file from download_url into local_dest\n",
        "    if the file doesn't already exists.\n",
        "    If expected_byte is provided, check if\n",
        "    the downloaded file has the same number of bytes.\n",
        "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
        "        print('%s already exists' %local_dest)\n",
        "    else:\n",
        "        print('Downloading %s' %download_url)\n",
        "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
        "        file_stat = os.stat(local_dest)\n",
        "        if expected_byte:\n",
        "            if file_stat.st_size == expected_byte:\n",
        "                print('Successfully downloaded %s' %local_dest)\n",
        "                if unzip_and_remove:\n",
        "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                    os.remove(local_dest)\n",
        "            else:\n",
        "                print('The downloaded file has unexpected number of bytes')\n",
        "\n",
        "def download_mnist(path):\n",
        "    \"\"\"\n",
        "    Download and unzip the dataset mnist if it's not already downloaded\n",
        "    Download from http://yann.lecun.com/exdb/mnist\n",
        "    \"\"\"\n",
        "    safe_mkdir(path)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist'\n",
        "    filenames = ['train-images-idx3-ubyte.gz',\n",
        "                'train-labels-idx1-ubyte.gz',\n",
        "                't10k-images-idx3-ubyte.gz',\n",
        "                't10k-labels-idx1-ubyte.gz']\n",
        "    expected_bytes = [9912422, 28881, 1648877, 4542]\n",
        "\n",
        "    for filename, byte in zip(filenames, expected_bytes):\n",
        "        download_url = os.path.join(url, filename)\n",
        "        local_dest = os.path.join(path, filename)\n",
        "        download_one_file(download_url, local_dest, byte, True)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "\n",
        "def read_mnist(path, flatten=True, num_train=55000):\n",
        "    \"\"\"\n",
        "    Read in the mnist dataset, given that the data is stored in path\n",
        "    Return two tuples of numpy arrays\n",
        "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "    \"\"\"\n",
        "    imgs, labels = parse_data(path, 'train', flatten)\n",
        "    indices = np.random.permutation(labels.shape[0])\n",
        "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
        "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
        "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return (train_img, train_labels), (val_img, val_labels), test\n",
        "\n",
        "\n",
        "def get_mnist_dataset(batch_size):\n",
        "    # Step 1: Read in data\n",
        "    mnist_folder = 'data/mnist'\n",
        "    download_mnist(mnist_folder)\n",
        "    train, val, test = read_mnist(mnist_folder, flatten=False)\n",
        "\n",
        "    # Step 2: Create datasets and iterator\n",
        "    train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "    train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
        "    train_data = train_data.batch(batch_size)\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "    test_data = test_data.batch(batch_size)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8)  # int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "\n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows,\n",
        "                                                         cols)  # uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "\n",
        "def show(image):\n",
        "    \"\"\"\n",
        "    Render a given numpy.uint8 2D array of pixel data.\n",
        "    \"\"\"\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "def huber_loss(labels, predictions, delta=14.0):\n",
        "    residual = tf.abs(labels - predictions)\n",
        "    def f1(): return 0.5 * tf.square(residual)\n",
        "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
        "    return tf.cond(residual < delta, f1, f2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqUDkA-2BtTy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# import utils\n",
        "\n",
        "#parameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "n_epochs = 100\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "n_nodes = 1024\n",
        "\n",
        "# step 1 : Read data\n",
        "mnist_folder = 'data/mnist'\n",
        "download_mnist(mnist_folder)\n",
        "train, val, test = read_mnist(mnist_folder, flatten=True)\n",
        "\n",
        "\n",
        "# step2 : create dataset and iterators\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "train_data = train_data.shuffle(1000)\n",
        "train_data = train_data.batch(batch_size)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "test_data = test_data.batch(batch_size)\n",
        "\n",
        "# create one iteration and initialize it from different dataset\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types,\n",
        "                                           train_data.output_shapes)\n",
        "img, label = iterator.get_next()\n",
        "\n",
        "train_init = iterator.make_initializer(train_data) # initializer for traindata\n",
        "test_init = iterator.make_initializer(test_data)  #\n",
        "\n",
        "# step 3 : initialize weight and bias\n",
        "# weight : initiali weight mean : 0 , std-dev : 0.1\n",
        "# tf.random_normal_initializer()\n",
        "# bias : 0.0 , tf.zeros_initializer()\n",
        "# img = (?,784) * weight = (784,10) = (?,10) + bias (1,10)\n",
        "print(img.shape, label.shape)\n",
        "w1 = tf.get_variable(\"weight1\", shape=(int(img.shape[1]), n_nodes),\n",
        "                     initializer=tf.random_normal_initializer(mean=0,\n",
        "                                                              stddev=0.01))\n",
        "b1 = tf.get_variable(\"bias1\", shape=(1, n_nodes), initializer=\n",
        "                     tf.zeros_initializer())\n",
        "\n",
        "w2 = tf.get_variable(\"weight2\", shape=(n_nodes, int(label.shape[1])),\n",
        "                     initializer=tf.random_normal_initializer(mean=0,\n",
        "                                                              stddev=0.01))\n",
        "b2 = tf.get_variable(\"bias2\", shape=(1, int(label.shape[1])), initializer=\n",
        "                     tf.zeros_initializer())\n",
        "\n",
        "# step 4: model , logits , model that return logits\n",
        "logits1 = tf.add(tf.matmul(img, w1), b1)\n",
        "relu_layer1 = tf.nn.relu(logits1)\n",
        "\n",
        "logits2 = tf.add(tf.matmul(relu_layer1, w2), b2)\n",
        "relu_layer2 = tf.nn.relu(logits2)\n",
        "\n",
        "\n",
        "# step 5 : entropy and loss\n",
        "entropy1 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=relu_layer2,\n",
        "                                                      labels=label,\n",
        "                                                      name=\"loss\")\n",
        "loss = tf.reduce_mean(entropy1)\n",
        "\n",
        "\n",
        "# adding regularization\n",
        "beta = 0.01\n",
        "regularizer = tf.nn.l2_loss(w2)\n",
        "loss = tf.reduce_mean(loss + beta * regularizer)\n",
        "\n",
        "\n",
        "\n",
        "#step 6 : optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) #\n",
        "\n",
        "\n",
        "correct_predictions = tf.equal(tf.argmax(relu_layer2, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "\n",
        "with tf.name_scope(\"summary_loss\"):\n",
        "    tf.summary.scalar(\"loss\", loss)\n",
        "    tf.summary.histogram(\"loss histogram\", loss)\n",
        "    summary_op1 = tf.summary.merge_all()\n",
        "\n",
        "with tf.name_scope(\"summary_accuracy\"):\n",
        "    tf.summary.scalar(\"loss\", accuracy)\n",
        "    tf.summary.histogram(\"loss histogram\", accuracy)\n",
        "    summary_op2 = tf.summary.merge_all()\n",
        "\n",
        "writer = tf.summary.FileWriter(\"./graphs/logdatareg\", tf.get_default_graph())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    start_time = time.time()\n",
        "    for i in range(n_epochs):\n",
        "        sess.run(train_init)\n",
        "        total_loss = 0\n",
        "        n_batches =0\n",
        "        try:\n",
        "            while True:\n",
        "                _, _loss, summary = sess.run([optimizer, loss, summary_op1])\n",
        "                total_loss +=_loss\n",
        "                n_batches +=1\n",
        "\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        writer.add_summary(summary, global_step=i)\n",
        "        print('Average loss epoch {0} : {1}'.format(i, total_loss/n_batches))\n",
        "    print(\"total time: {0}\".format(time.time() - start_time))\n",
        "\n",
        "    # for accuracy\n",
        "    sess.run(test_init)\n",
        "    total_correct_pred =0\n",
        "    i = 0\n",
        "    try:\n",
        "        while True:\n",
        "            accuracy_batch, summary = sess.run([accuracy, summary_op2])\n",
        "            total_correct_pred += accuracy_batch\n",
        "            i += 1\n",
        "            writer.add_summary(summary, global_step=i)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "    print(\"ACCURACY {0} \".format(total_correct_pred/n_test))\n",
        "    writer.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}