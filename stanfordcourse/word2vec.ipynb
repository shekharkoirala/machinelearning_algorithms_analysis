{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharkoirala/machinelearning_algorithms_analysis/blob/master/stanfordcourse/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rFE4FPHXZsNo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQT20oo7Z-jW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyper parameters \n",
        "VOCAB_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
        "SKIP_WINDOW = 1             # the context window\n",
        "NUM_SAMPLED = 64            # number of negative examples to sample\n",
        "LEARNING_RATE = 1.0\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "VISUAL_FLD = 'visualization'\n",
        "SKIP_STEP = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48qX303xaCpy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
        "EXPECTED_BYTES = 31344016\n",
        "NUM_VISUALIZE = 3000   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6m6UUfVqaE9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd37ae03-e040-4d78-840a-55fb98a9892a"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data  visualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-fK7m26qa_--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Prepare data** \n",
        "***this works , when word2veceager notebooks prepare data executed ( for the first time)***"
      ]
    },
    {
      "metadata": {
        "id": "bQtp_JMnbLnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "file_path = \"data/text8.zip\"\n",
        "with zipfile.ZipFile(file_path) as f:\n",
        "  words = tf.compat.as_str(f.read(f.namelist()[0])).split() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOyOfQJAbYqO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "dictionary = dict()\n",
        "count = [('UNK', -1)]\n",
        "index = 0\n",
        "count.extend(Counter(words).most_common(VOCAB_SIZE - 1))\n",
        "for word, _ in count:\n",
        "  dictionary[word] = index\n",
        "  index += 1\n",
        "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msQOOUYpcGEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_words =[dictionary[word] if word in dictionary else 0 for word in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQomyI_xcKuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# del words , since google collab\n",
        "\n",
        "#based on skip gram\n",
        "\n",
        "import random\n",
        "def generate_sample(index_words , context_window_size):\n",
        "  #\"made according to skip gram , each target context pair is treated as new data\"\n",
        "  for index, center in enumerate(index_words):\n",
        "      #\"center is index from dictionary and we need index to calculate index words\"\n",
        "      context = random.randint(1,context_window_size)\n",
        "      # context is random , since context_window_size is 1 , it is always 1\n",
        "      # before the center words\n",
        "      for target in index_words[max(0, index-context):index]:\n",
        "        yield center , target\n",
        "      # after the center words\n",
        "      for target in index_words[index+1:index+1+context]:\n",
        "        yield center , target\n",
        "\n",
        "simple_gen = generate_sample(index_words, context_window_size = SKIP_WINDOW)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvnRihQ7cPqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46f949a0-05cd-4c90-e833-58899bca8016"
      },
      "cell_type": "code",
      "source": [
        "len(words), len(dictionary)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17005207, 50000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Iq9fsaUicTEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8cb0e36-56cd-4774-d42a-92c5f77ef0bf"
      },
      "cell_type": "code",
      "source": [
        "next(simple_gen) # will print (5234,?) (5234,?) two times "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5234, 3081)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "ZEA3XcfXcVp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba8bc42a-6682-444f-9c0e-13b5165940b3"
      },
      "cell_type": "code",
      "source": [
        "def batch_gen():\n",
        "  simple_gen = generate_sample(index_words, context_window_size= SKIP_WINDOW)\n",
        "  while True:\n",
        "    center_batch= np.zeros(BATCH_SIZE, dtype= np.int32)\n",
        "    target_batch= np.zeros([BATCH_SIZE, 1])\n",
        "#     print(center_batch.shape, target_batch.shape)\n",
        "    for index in range(BATCH_SIZE):\n",
        "      center_batch[index], target_batch[index] = next(simple_gen)\n",
        "    yield center_batch, target_batch\n",
        "\n",
        "batch_gen()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object batch_gen at 0x7f120a61eeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "GtYNohYvkNzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.mkdir(\"checkpoints\")\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MToPj2gycaDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class word2Vec:\n",
        "  def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate, dataset):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_sampled = num_sampled\n",
        "    self.embed_size = embed_size\n",
        "    self.batch_size = batch_size\n",
        "    self.num_sampled= num_sampled\n",
        "    self.learning_rate= learning_rate\n",
        "    self.dataset = dataset\n",
        "    self.skip_step = SKIP_STEP\n",
        "    self.global_step = tf.get_variable('global_step',\n",
        "                                       initializer=tf.constant(0),\n",
        "                                       trainable=False)\n",
        "    \n",
        "  def _import_data(self):\n",
        "    #initialize data\n",
        "    with tf.name_scope(\"data\"):\n",
        "      self.iterator = self.dataset.make_initializable_iterator()\n",
        "      self.center_words, self.target_words = self.iterator.get_next()\n",
        "      \n",
        "\n",
        "   \n",
        "  \n",
        "    \n",
        "  def _create_embedding(self):\n",
        "    #embedding setup\n",
        "    with tf.name_scope(\"embed\"):\n",
        "      self.embed_matrix = tf.get_variable(\"embed_matrix\", shape=[self.vocab_size,\n",
        "                                                            self.embed_size],\n",
        "                                    initializer=tf.random_uniform_initializer())\n",
        "      #compute forward pass of word2vec with NCE loss\n",
        "      self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words,\n",
        "                                     name=\"embedding\")\n",
        "  \n",
        "  def _create_loss(self):\n",
        "    #define loss\n",
        "    with tf.name_scope(\"loss\"):\n",
        "      nce_weights = tf.get_variable(\"nce_weights\",\n",
        "                                         shape=[self.vocab_size,self.embed_size],\n",
        "                                         initializer=\n",
        "                                         tf.truncated_normal_initializer(\n",
        "                                             stddev=1.0/(self.embed_size ** 0.5)))\n",
        "      nce_biases = tf.get_variable(\"nce_biases\",\n",
        "                                        initializer= tf.zeros([self.vocab_size]))\n",
        "      self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                               biases=nce_biases,\n",
        "                                               labels=self.target_words,\n",
        "                                               inputs=self.embed,\n",
        "                                               num_sampled=self.num_sampled,\n",
        "                                               num_classes=self.vocab_size),\n",
        "                                name=\"loss\")\n",
        "   \n",
        "      \n",
        "  def _create_optimizer(self):\n",
        "    #define optimizer \n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss,\n",
        "                                                                          global_step=self.global_step)\n",
        "    \n",
        "    \n",
        "  def _create_summaries(self):\n",
        "    with tf.name_scope(\"summaries\"):\n",
        "      tf.summary.scalar('loss', self.loss)\n",
        "      tf.summary.histogram(\"histogram_loss\", self.loss)\n",
        "      self.summary_op = tf.summary.merge_all()\n",
        "  \n",
        "  def build_graph(self):\n",
        "    #phase1\n",
        "    #step 1 create dataset and samples\n",
        "    self._import_data()\n",
        "    #step 2 create embedding matrix, and inference ( compute forward path )\n",
        "    self._create_embedding()\n",
        "    #step 3 loss function\n",
        "    self._create_loss()\n",
        "    #step 4 optimizer and train instance\n",
        "    self._create_optimizer()\n",
        "    self._create_summaries()\n",
        "\n",
        "  def main_process(self, num_tain_steps):\n",
        "    #phase 2\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    initial_step = 0\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(self.iterator.initializer)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      summary_writer = tbc.get_writer()\n",
        "      writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "      ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "      \n",
        "      if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      \n",
        "      total_loss = 0.0\n",
        "      writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "      summary_writer.add_graph(sess.graph)\n",
        "      initial_step = self.global_step.eval()\n",
        "      \n",
        "      for index in range(initial_step, initial_step + NUM_TRAIN_STEPS):\n",
        "        try:\n",
        "          loss_batch, _, summary = sess.run([self.loss,self.optimizer, self.summary_op])\n",
        "          writer.add_summary(summary, global_step=index)\n",
        "          summary_writer.add_summary(summary, global_step=index)\n",
        "          total_loss+=loss_batch\n",
        "          \n",
        "          if (index +1)% self.skip_step ==0:\n",
        "            print('Average loss at step {}: {:5.1f}'.format(index, total_loss/self.skip_step))\n",
        "            total_loss =0.0\n",
        "            saver.save(sess, 'checkpoints/skip-gram',index)\n",
        "            \n",
        "        except tf.errors.OutOfRangeError:\n",
        "          sess.run(self.iterator.initializer)\n",
        "      writer.close()\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r_7Gc5bHDdO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c3169f36-a128-44d6-8fd8-d77859cbcd42"
      },
      "cell_type": "code",
      "source": [
        "import tensorboardcolab as tb\n",
        "tbc = tb.TensorBoardColab()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://df54983e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HJIvch60ccav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a384328f-6094-45da-b06e-64d9bb6fcf93"
      },
      "cell_type": "code",
      "source": [
        "def data_generator():\n",
        "  yield from batch_gen()\n",
        "\n",
        "def main__():\n",
        "  tf.reset_default_graph()\n",
        "  dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32, tf.int32),\n",
        "                                           (tf.TensorShape([BATCH_SIZE]),\n",
        "                                           tf.TensorShape([BATCH_SIZE,1])))\n",
        "  \n",
        "#   vocab_size, embed_size, batch_size, num_sampled, learning_rate, dataset\n",
        "  W2V = word2Vec(vocab_size=VOCAB_SIZE,\n",
        "                 embed_size=EMBED_SIZE,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 num_sampled=NUM_SAMPLED,\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 dataset=dataset)\n",
        "  W2V.build_graph()\n",
        "  W2V.main_process(NUM_TRAIN_STEPS)\n",
        "  \n",
        "main__()"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss at step 4999:  65.6\n",
            "Average loss at step 9999:  18.3\n",
            "Average loss at step 14999:   9.6\n",
            "Average loss at step 19999:   6.6\n",
            "Average loss at step 24999:   5.6\n",
            "Average loss at step 29999:   5.3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Average loss at step 34999:   5.0\n",
            "Average loss at step 39999:   4.8\n",
            "Average loss at step 44999:   4.8\n",
            "Average loss at step 49999:   4.8\n",
            "Average loss at step 54999:   4.7\n",
            "Average loss at step 59999:   4.7\n",
            "Average loss at step 64999:   4.6\n",
            "Average loss at step 69999:   4.7\n",
            "Average loss at step 74999:   4.6\n",
            "Average loss at step 79999:   4.6\n",
            "Average loss at step 84999:   4.7\n",
            "Average loss at step 89999:   4.6\n",
            "Average loss at step 94999:   4.6\n",
            "Average loss at step 99999:   4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "35vHkH5BodBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#if error, or skip\n",
        "!mkdir graphs/word2vec/lr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nkQphf-SBYyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05eacfab-c4de-445e-8487-978a871c7414"
      },
      "cell_type": "code",
      "source": [
        "!rm checkpoints/*"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'checkpoints/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z0EJ-1Gvbvru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyybItCkqH3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf graphs/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L37ZR9hjb3fl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VT-8iHhFqD3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}