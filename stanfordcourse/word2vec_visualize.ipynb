{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_visualize.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharkoirala/machinelearning_algorithms_analysis/blob/master/stanfordcourse/word2vec_visualize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p-nI81myPXwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ns-trBIqPsVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyper parameters \n",
        "VOCAB_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
        "SKIP_WINDOW = 1             # the context window\n",
        "NUM_SAMPLED = 64            # number of negative examples to sample\n",
        "LEARNING_RATE = 1.0\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "VISUAL_FLD = 'visualization'\n",
        "SKIP_STEP = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQ6DPSvaP04W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
        "EXPECTED_BYTES = 31344016\n",
        "NUM_VISUALIZE = 3000   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rgnNsZGiP4_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a18527c-19be-478c-e57d-e768048a325a"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints  data  Graph  graphs  sample_data  visualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xtAFsfyAP-T-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Prepare data** \n",
        "***this works , when word2veceager notebooks prepare data executed ( for the first time)***"
      ]
    },
    {
      "metadata": {
        "id": "UlPxu-zNP5pf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "file_path = \"data/text8.zip\"\n",
        "with zipfile.ZipFile(file_path) as f:\n",
        "  words = tf.compat.as_str(f.read(f.namelist()[0])).split() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XhHnZKOoP-CG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "dictionary = dict()\n",
        "count = [('UNK', -1)]\n",
        "index = 0\n",
        "count.extend(Counter(words).most_common(VOCAB_SIZE - 1))\n",
        "for word, _ in count:\n",
        "  dictionary[word] = index\n",
        "  index += 1\n",
        "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Oy0e9aSQCS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_words =[dictionary[word] if word in dictionary else 0 for word in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPwhhK16QEG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# del words , since google collab\n",
        "\n",
        "#based on skip gram\n",
        "\n",
        "import random\n",
        "def generate_sample(index_words , context_window_size):\n",
        "  #\"made according to skip gram , each target context pair is treated as new data\"\n",
        "  for index, center in enumerate(index_words):\n",
        "      #\"center is index from dictionary and we need index to calculate index words\"\n",
        "      context = random.randint(1,context_window_size)\n",
        "      # context is random , since context_window_size is 1 , it is always 1\n",
        "      # before the center words\n",
        "      for target in index_words[max(0, index-context):index]:\n",
        "        yield center , target\n",
        "      # after the center words\n",
        "      for target in index_words[index+1:index+1+context]:\n",
        "        yield center , target\n",
        "\n",
        "simple_gen = generate_sample(index_words, context_window_size = SKIP_WINDOW)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BhEpSTFQFqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ed58c91-2f2a-430d-ffc1-9543a6149b87"
      },
      "cell_type": "code",
      "source": [
        "len(words), len(dictionary)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17005207, 50000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "949YdGD5QHSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f40cbf8c-85d6-4deb-8f44-e33ec3c14c3b"
      },
      "cell_type": "code",
      "source": [
        "def batch_gen():\n",
        "  simple_gen = generate_sample(index_words, context_window_size= SKIP_WINDOW)\n",
        "  while True:\n",
        "    center_batch= np.zeros(BATCH_SIZE, dtype= np.int32)\n",
        "    target_batch= np.zeros([BATCH_SIZE, 1])\n",
        "#     print(center_batch.shape, target_batch.shape)\n",
        "    for index in range(BATCH_SIZE):\n",
        "      center_batch[index], target_batch[index] = next(simple_gen)\n",
        "    yield center_batch, target_batch\n",
        "\n",
        "batch_gen()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object batch_gen at 0x7f3119e806d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "1CleDpXKQI8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class word2Vec:\n",
        "  def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate, dataset):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_sampled = num_sampled\n",
        "    self.embed_size = embed_size\n",
        "    self.batch_size = batch_size\n",
        "    self.num_sampled= num_sampled\n",
        "    self.learning_rate= learning_rate\n",
        "    self.dataset = dataset\n",
        "    self.skip_step = SKIP_STEP\n",
        "    self.global_step = tf.get_variable('global_step',\n",
        "                                       initializer=tf.constant(0),\n",
        "                                       trainable=False)\n",
        "    \n",
        "  def _import_data(self):\n",
        "    #initialize data\n",
        "    with tf.name_scope(\"data\"):\n",
        "      self.iterator = self.dataset.make_initializable_iterator()\n",
        "      self.center_words, self.target_words = self.iterator.get_next()\n",
        "      \n",
        "\n",
        "   \n",
        "  \n",
        "    \n",
        "  def _create_embedding(self):\n",
        "    #embedding setup\n",
        "    with tf.name_scope(\"embed\"):\n",
        "      self.embed_matrix = tf.get_variable(\"embed_matrix\", shape=[self.vocab_size,\n",
        "                                                            self.embed_size],\n",
        "                                    initializer=tf.random_uniform_initializer())\n",
        "      #compute forward pass of word2vec with NCE loss\n",
        "      self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words,\n",
        "                                     name=\"embedding\")\n",
        "  \n",
        "  def _create_loss(self):\n",
        "    #define loss\n",
        "    with tf.name_scope(\"loss\"):\n",
        "      nce_weights = tf.get_variable(\"nce_weights\",\n",
        "                                         shape=[self.vocab_size,self.embed_size],\n",
        "                                         initializer=\n",
        "                                         tf.truncated_normal_initializer(\n",
        "                                             stddev=1.0/(self.embed_size ** 0.5)))\n",
        "      nce_biases = tf.get_variable(\"nce_biases\",\n",
        "                                        initializer= tf.zeros([self.vocab_size]))\n",
        "      self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                               biases=nce_biases,\n",
        "                                               labels=self.target_words,\n",
        "                                               inputs=self.embed,\n",
        "                                               num_sampled=self.num_sampled,\n",
        "                                               num_classes=self.vocab_size),\n",
        "                                name=\"loss\")\n",
        "   \n",
        "      \n",
        "  def _create_optimizer(self):\n",
        "    #define optimizer \n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss,\n",
        "                                                                          global_step=self.global_step)\n",
        "    \n",
        "    \n",
        "  def _create_summaries(self):\n",
        "    with tf.name_scope(\"summaries\"):\n",
        "      tf.summary.scalar('loss', self.loss)\n",
        "      tf.summary.histogram(\"histogram_loss\", self.loss)\n",
        "      self.summary_op = tf.summary.merge_all()\n",
        "  \n",
        "  def build_graph(self):\n",
        "    #phase1\n",
        "    #step 1 create dataset and samples\n",
        "    self._import_data()\n",
        "    #step 2 create embedding matrix, and inference ( compute forward path )\n",
        "    self._create_embedding()\n",
        "    #step 3 loss function\n",
        "    self._create_loss()\n",
        "    #step 4 optimizer and train instance\n",
        "    self._create_optimizer()\n",
        "    self._create_summaries()\n",
        "\n",
        "  def train(self, num_tain_steps):\n",
        "    #phase 2\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    initial_step = 0\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(self.iterator.initializer)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      summary_writer = tbc.get_writer()\n",
        "      writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "      ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "      \n",
        "      if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      \n",
        "      total_loss = 0.0\n",
        "      writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "      summary_writer.add_graph(sess.graph)\n",
        "      initial_step = self.global_step.eval()\n",
        "      \n",
        "      for index in range(initial_step, initial_step + NUM_TRAIN_STEPS):\n",
        "        try:\n",
        "          loss_batch, _, summary = sess.run([self.loss,self.optimizer, self.summary_op])\n",
        "          writer.add_summary(summary, global_step=index)\n",
        "          summary_writer.add_summary(summary, global_step=index)\n",
        "          total_loss+=loss_batch\n",
        "          \n",
        "          if (index +1)% self.skip_step ==0:\n",
        "            print('Average loss at step {}: {:5.1f}'.format(index, total_loss/self.skip_step))\n",
        "            total_loss =0.0\n",
        "            saver.save(sess, 'checkpoints/skip-gram',index)\n",
        "            \n",
        "        except tf.errors.OutOfRangeError:\n",
        "          sess.run(self.iterator.initializer)\n",
        "      writer.close()\n",
        "      \n",
        "  def visualize(self,visual_fld):\n",
        "    words = open(os.path.join(visual_fld, 'vocab.tsv'), 'r').readlines()[:num_visualize]\n",
        "    words = [word for word in words]\n",
        "    file = open(os.path.join(visual_fld, 'vocab_' + str(num_visualize) + '.tsv'), 'w')\n",
        "    for word in words:\n",
        "        file.write(word)\n",
        "    file.close()\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "\n",
        "      # if that checkpoint exists, restore from checkpoint\n",
        "      if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "      final_embed_matrix = sess.run(self.embed_matrix)\n",
        "    \n",
        "      embedding_var = tf.Variable(final_embed_matrix[:num_visualize], name='embedding')\n",
        "      sess.run(embedding_var.initializer)\n",
        "\n",
        "      config = projector.ProjectorConfig()\n",
        "      summary_writer = tf.summary.FileWriter(visual_fld)\n",
        "\n",
        "      # add embedding to the config file\n",
        "      embedding = config.embeddings.add()\n",
        "      embedding.tensor_name = embedding_var.name\n",
        "\n",
        "      # link this tensor to its metadata file, in this case the first NUM_VISUALIZE words of vocab\n",
        "      embedding.metadata_path = 'vocab_' + str(num_visualize) + '.tsv'\n",
        "\n",
        "      # saves a configuration file that TensorBoard will read during startup.\n",
        "      projector.visualize_embeddings(summary_writer, config)\n",
        "      saver_embed = tf.train.Saver([embedding_var])\n",
        "      saver_embed.save(sess, os.path.join(visual_fld, 'model.ckpt'), 1)\n",
        "      \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgEqba9EQLit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e98d83a9-a78b-42c7-b0f4-b0a8af010d09"
      },
      "cell_type": "code",
      "source": [
        "import tensorboardcolab as tb\n",
        "tbc = tb.TensorBoardColab()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://5a37529e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l6QCZh-gvVi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "800f7d52-3980-46c7-9ba0-5e654eaba3c2"
      },
      "cell_type": "code",
      "source": [
        "def data_generator():\n",
        "  yield from batch_gen()\n",
        "\n",
        "def main__():\n",
        "  tf.reset_default_graph()\n",
        "  dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32, tf.int32),\n",
        "                                           (tf.TensorShape([BATCH_SIZE]),\n",
        "                                           tf.TensorShape([BATCH_SIZE,1])))\n",
        "  \n",
        "#   vocab_size, embed_size, batch_size, num_sampled, learning_rate, dataset\n",
        "  W2V = word2Vec(vocab_size=VOCAB_SIZE,\n",
        "                 embed_size=EMBED_SIZE,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 num_sampled=NUM_SAMPLED,\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 dataset=dataset)\n",
        "  W2V.build_graph()\n",
        "  W2V.train(NUM_TRAIN_STEPS)\n",
        "#   W2V.visualize(visual_fld=VISUAL_FLD, NUM_VISUALIZE)\n",
        "  \n",
        "  \n",
        "main__()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-26-f21f1aef72b0>:18: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Average loss at step 4999:  65.0\n",
            "Average loss at step 9999:  18.3\n",
            "Average loss at step 14999:   9.8\n",
            "Average loss at step 19999:   6.7\n",
            "Average loss at step 24999:   5.7\n",
            "Average loss at step 29999:   5.3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Average loss at step 34999:   5.0\n",
            "Average loss at step 39999:   4.8\n",
            "Average loss at step 44999:   4.8\n",
            "Average loss at step 49999:   4.8\n",
            "Average loss at step 54999:   4.8\n",
            "Average loss at step 59999:   4.7\n",
            "Average loss at step 64999:   4.6\n",
            "Average loss at step 69999:   4.7\n",
            "Average loss at step 74999:   4.6\n",
            "Average loss at step 79999:   4.7\n",
            "Average loss at step 84999:   4.7\n",
            "Average loss at step 89999:   4.7\n",
            "Average loss at step 94999:   4.6\n",
            "Average loss at step 99999:   4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "53qYM_RhvfKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm checkpoints/*\n",
        "!rm -rf graphs/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdQERsAGvfyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}