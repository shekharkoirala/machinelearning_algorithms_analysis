{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_visualize.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharkoirala/machinelearning_algorithms_analysis/blob/master/stanfordcourse/word2vec_visualize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p-nI81myPXwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ns-trBIqPsVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyper parameters \n",
        "VOCAB_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
        "SKIP_WINDOW = 1             # the context window\n",
        "NUM_SAMPLED = 64            # number of negative examples to sample\n",
        "LEARNING_RATE = 1.0\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "VISUAL_FLD = 'visualization'\n",
        "SKIP_STEP = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQ6DPSvaP04W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
        "EXPECTED_BYTES = 31344016\n",
        "NUM_VISUALIZE = 3000   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rgnNsZGiP4_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dec0b638-fe7e-4831-8235-edce7ab7ebf7"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints  data  Graph  graphs  sample_data  visualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xtAFsfyAP-T-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Prepare data** \n",
        "***this works , when word2veceager notebooks prepare data executed ( for the first time)***"
      ]
    },
    {
      "metadata": {
        "id": "UlPxu-zNP5pf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "file_path = \"data/text8.zip\"\n",
        "with zipfile.ZipFile(file_path) as f:\n",
        "  words = tf.compat.as_str(f.read(f.namelist()[0])).split() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XhHnZKOoP-CG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "dictionary = dict()\n",
        "count = [('UNK', -1)]\n",
        "index = 0\n",
        "count.extend(Counter(words).most_common(VOCAB_SIZE - 1))\n",
        "for word, _ in count:\n",
        "  dictionary[word] = index\n",
        "  index += 1\n",
        "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Oy0e9aSQCS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_words =[dictionary[word] if word in dictionary else 0 for word in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPwhhK16QEG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# del words , since google collab\n",
        "\n",
        "#based on skip gram\n",
        "\n",
        "import random\n",
        "def generate_sample(index_words , context_window_size):\n",
        "  #\"made according to skip gram , each target context pair is treated as new data\"\n",
        "  for index, center in enumerate(index_words):\n",
        "      #\"center is index from dictionary and we need index to calculate index words\"\n",
        "      context = random.randint(1,context_window_size)\n",
        "      # context is random , since context_window_size is 1 , it is always 1\n",
        "      # before the center words\n",
        "      for target in index_words[max(0, index-context):index]:\n",
        "        yield center , target\n",
        "      # after the center words\n",
        "      for target in index_words[index+1:index+1+context]:\n",
        "        yield center , target\n",
        "\n",
        "simple_gen = generate_sample(index_words, context_window_size = SKIP_WINDOW)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BhEpSTFQFqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dffcc81c-c852-40db-aedf-44a77cdaddc2"
      },
      "cell_type": "code",
      "source": [
        "len(words), len(dictionary)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17005207, 50000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "949YdGD5QHSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38d48829-f11f-45d5-ba54-6cc6f0038d0c"
      },
      "cell_type": "code",
      "source": [
        "def batch_gen():\n",
        "  simple_gen = generate_sample(index_words, context_window_size= SKIP_WINDOW)\n",
        "  while True:\n",
        "    center_batch= np.zeros(BATCH_SIZE, dtype= np.int32)\n",
        "    target_batch= np.zeros([BATCH_SIZE, 1])\n",
        "#     print(center_batch.shape, target_batch.shape)\n",
        "    for index in range(BATCH_SIZE):\n",
        "      center_batch[index], target_batch[index] = next(simple_gen)\n",
        "    yield center_batch, target_batch\n",
        "\n",
        "batch_gen()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object batch_gen at 0x7f3119e80e60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "1CleDpXKQI8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class word2Vec:\n",
        "  def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate, dataset):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_sampled = num_sampled\n",
        "    self.embed_size = embed_size\n",
        "    self.batch_size = batch_size\n",
        "    self.num_sampled= num_sampled\n",
        "    self.learning_rate= learning_rate\n",
        "    self.dataset = dataset\n",
        "    self.skip_step = SKIP_STEP\n",
        "    self.global_step = tf.get_variable('global_step',\n",
        "                                       initializer=tf.constant(0),\n",
        "                                       trainable=False)\n",
        "    \n",
        "  def _import_data(self):\n",
        "    #initialize data\n",
        "    with tf.name_scope(\"data\"):\n",
        "      self.iterator = self.dataset.make_initializable_iterator()\n",
        "      self.center_words, self.target_words = self.iterator.get_next()\n",
        "      \n",
        "    with tf.name_scope(\"nce\"):\n",
        "      self.nce_weights = tf.get_variable(\"nce_weights\",\n",
        "                                         shape=[self.vocab_size,self.embed_size],\n",
        "                                         initializer=\n",
        "                                         tf.truncated_normal_initializer(\n",
        "                                             stddev=1.0/(self.embed_size ** 0.5)))\n",
        "      self.nce_biases = tf.get_variable(\"nce_biases\",\n",
        "                                        initializer= tf.zeros([self.vocab_size]))\n",
        "   \n",
        "  \n",
        "    \n",
        "  def _create_embedding(self):\n",
        "    #embedding setup\n",
        "    with tf.name_scope(\"embed\"):\n",
        "      self.embed_matrix = tf.get_variable(\"embed_matrix\", shape=[self.vocab_size,\n",
        "                                                            self.embed_size],\n",
        "                                    initializer=tf.random_uniform_initializer())\n",
        "      #compute forward pass of word2vec with NCE loss\n",
        "      self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words,\n",
        "                                     name=\"embedding\")\n",
        "    pass\n",
        "  \n",
        "  def _create_loss(self):\n",
        "    #define loss\n",
        "    with tf.name_scope(\"loss\"):\n",
        "      self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=self.nce_weights,\n",
        "                                               biases=self.nce_biases,\n",
        "                                               labels=self.target_words,\n",
        "                                               inputs=self.embed,\n",
        "                                               num_sampled=self.num_sampled,\n",
        "                                               num_classes=self.vocab_size),\n",
        "                                name=\"loss\")\n",
        "   \n",
        "      \n",
        "  def _create_optimizer(self):\n",
        "    #define optimizer \n",
        "    with tf.name_scope(\"optimizer\"):\n",
        "      self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "    \n",
        "    \n",
        "  def _create_summaries(self):\n",
        "    with tf.name_scope(\"summaries\"):\n",
        "      tf.summary.scalar('loss', self.loss)\n",
        "      tf.summary.histogram(\"histogram_loss\", self.loss)\n",
        "      self.summary_op = tf.summary.merge_all()\n",
        "  \n",
        "  def build_graph(self):\n",
        "    #phase1\n",
        "    #step 1 create dataset and samples\n",
        "    self._import_data()\n",
        "    #step 2 create embedding matrix, and inference ( compute forward path )\n",
        "    self._create_embedding()\n",
        "    #step 3 loss function\n",
        "    self._create_loss()\n",
        "    #step 4 optimizer and train instance\n",
        "    self._create_optimizer()\n",
        "    self._create_summaries()\n",
        "\n",
        "  def main_process(self, num_tain_steps):\n",
        "    #phase 2\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    initial_step = 0\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(self.iterator.initializer)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      summary_writer = tbc.get_writer()\n",
        "      writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "      ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "      \n",
        "      if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      \n",
        "      total_loss = 0.0\n",
        "      writer = tf.summary.FileWriter('graphs', sess.graph)\n",
        "      summary_writer.add_graph(sess.graph)\n",
        "      initial_step = self.global_step.eval()\n",
        "      \n",
        "      for index in range(initial_step, initial_step + NUM_TRAIN_STEPS):\n",
        "        try:\n",
        "          loss_batch, _, summary = sess.run([self.loss,self.optimizer, self.summary_op])\n",
        "          writer.add_summary(summary, global_step=index)\n",
        "          summary_writer.add_summary(summary, global_step=index)\n",
        "          total_loss+=loss_batch\n",
        "          \n",
        "          if (index +1)% self.skip_step ==0:\n",
        "            print('Average loss at step {}: {:5.1f}'.format(index, total_loss/self.skip_step))\n",
        "            total_loss =0.0\n",
        "            saver.save(sess, 'checkpoints/skip-gram',index)\n",
        "            \n",
        "        except tf.errors.OutOfRangeError:\n",
        "          sess.run(self.iterator.initializer)\n",
        "      writer.close()\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgEqba9EQLit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}